{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikits de Python\n",
    "------\n",
    "\n",
    "Python posee una gran cantidad de paquetes open source dedicadas a diversos temas.\n",
    "En general se consideran add-ons de SciPy y Numpy, por lo mismo poseen dependencias a estas librerías\n",
    "\n",
    "Tienen aplicaciones en análisis financiero, procesamiento de audio, cómputo biomédico, machine learning, entre otros.\n",
    "\n",
    "Una lista de los paquetes disponibles se puede consultar en http://scikits.appspot.com/scikits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_scikit-aero:_ Cálculos de ingeniería aeronáutica <br>\n",
    "_scikit-image:_ Procesamiento de imágenes<br>\n",
    "_scikit-rf:_ Ingeniería de microondas<br>\n",
    "_audiolab:_ Procesamiento de audio<br>\n",
    "_timeseries:_ Manipulación de series temporales<br>\n",
    "_scikit-learn:_ Machine learning<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "Parte del éxito de Python se debe a haber sido adoptado como uno de los lenguajes más usados para métodos de aprendizaje automático e inteligencia artificial.\n",
    "Librerías como Keras y TensorFlow (Google) poseen todas sus funcionalidades en Python y son usadas por grandes empresas para predecir,sistemas de clasificación y recomendaciones a usuarios.\n",
    "\n",
    "La librería más común para Python por su simplicidad es scikit-learn, la cual posee módulos para pre-procesamiento de datos, extracción de características y algoritmos de aprendizaje automático.\n",
    "\n",
    "Machine Learning en primera instancia es un subconjunto de Inteligencia Artificial, en donde se comprende como la capacidad de un sistema de automatizar tareas que normalmente son desarrolladas por humanos; con esto comprendemos que existen algoritmos de Inteligencia Artificial que no necesariamente requieren de Machine Learning, algunos ejemplos de esto son lógica difusa e inteligencia simbólica.\n",
    "\n",
    "<img src='Class5D/ai_ml.jpg'>\n",
    "\n",
    "Machine Learning comprende un amplio conjunto de algoritmos en los cuales se busca que un modelo matemático logre mejorar su funcionamiento en el desempeño de una tarea asignada mediante reglas de aprendizaje. A diferencia de la programación habitual donde un algoritmo es propuesto para realizar una tarea, el modelo matemático es capaz de realizar la tarea solicitada después de iteraciones en donde intenta maximizar o minimizar alguna función de pérdida o ganancia. \n",
    "\n",
    "Un ejemplo de esto se puede ver en el caso de que se desee distinguir si una cadena de caractéres es un correo electrónico o no, en programación habitual el programador asigna reglas que se han de cumplir para definir si se trata de un correo o no, como lo son la presencia de ciertos caractéres o la presencia de cierto dominio en la cadena. En el caso de Machine Learning el algoritmo se le proporciona una base de datos con ejemplos de cadenas que son o no son correos electrónicos, una vez que ha sido entrenado el modelo, es capaz de distinguir a qué clase pertenece una cadena mediante características encontradas que permiten diferenciarlas.\n",
    "\n",
    "Los pasos típicos para entrenar un modelo son los siguientes:\n",
    "* Obtención de datos (preprocesamiento): En este paso los datos son obtenidos de un dataset y se realizan los procedimientos necesarios para hacer una limpieza de los datos o seleccionar con cuales datos se quiere trabajar, para esto se utilizan técnicas de visualización y análisis estadístico para remover muestras que han sido corrompidas o en las cuales se tienen datos faltantes.\n",
    "* Extracción de características: Una vez que se tienen los datos con los cuales se piensa trabajar, se puede hacer uso de técnicas de extracción de características para una mejor representación de los datos. E.g. si se tienen los datos de peso y altura de una persona y se intenta predecir si tiene obesidad, se pude obtener una característica como el Índice de Masa Corporal. \n",
    "* Entrenamiento del algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Paradigmas en Machine Learning\n",
    "\n",
    "Dentro de Machine Learning existen diferentes paradigmas sobre el tipo de modelo que se necesita para resolver el problema de automatizar alguna tarea. En ocasiones pueden ser usados en conjunto a manera de pasos sucesivos. Los paradigmas son:\n",
    "\n",
    "* Aprendizaje supervisado\n",
    "* Aprendizaje no supervisado\n",
    "* Aprendizaje por refuerzo\n",
    "\n",
    "<br>\n",
    "\n",
    "<p style=\"text-align:center\"> \n",
    "    <img src='Class5D/ml_1.jpg' height=\"240\" width=\"700\">\n",
    "</p>\n",
    "\n",
    "Existen diversas implementaciones de algoritmos de los dos primeros tipos en la librería de scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizaje supervisado\n",
    "\n",
    "El aprendizaje supervisado permite hacer uso de un set de datos previamente etiquetados de los cuales posee atributos que se desean predecir conociendo sus atributos. Dos tareas típicas para esto son:\n",
    "\n",
    "* Clasificación: Se tienen diferentes categorías a las cuales pueden pertenecer cada uno de los ejemplos dentro de un set de datos, las características son mostradas como un vector numérico, en el cual cada una de las dimensiones representa una característica.   \n",
    "El modelo final ha de ser capaz de clasificar nuevos datos no presentes entre los que han sido usados para el entrenamiento.\n",
    "Las salidas pertenecen a un set discreto de valores.\n",
    "\n",
    "\n",
    "* Regresión: Se tienen diversos ejemplos en un dataset del cual se poseen diferentes atributos para cada uno, se plantea predecir un valor numérico a la salida del modelo. E.g. intentar predecir el precio de una casa conociendo características sobre ella.\n",
    "Las salidas pertenecen a un set continuo de valores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión lineal\n",
    "\n",
    "Uno de los algoritmos más básicos y del cual se derivan otros es la regresión lineal.   \n",
    "El método más común para resolverlo es por gradiente descendiente.  \n",
    "El cual se expresa a continuación:\n",
    "\n",
    "$\\hat{y} = \\theta x$\n",
    "<br>\n",
    "$\\theta_{t+1} = \\theta_{t} - \\alpha  \\sum{(\\hat{y}(x_{i}) - y_i)} x_{i}$\n",
    "\n",
    "Donde $x$ y $y$ son variables conocidas y suponemos que y depende de $x$, $\\theta$ son los parámetros del modelo, $\\hat{y}$ es la salida estimada y $\\alpha$ es un factor de actualización.\n",
    "\n",
    "La función de actualización es consecuencia de la función de costo de mínimos cuadrados. <br>\n",
    "\n",
    "$\\hat{\\theta}=argmin([\\theta X - Y]^{2})$ <br>\n",
    "$\\hat{\\theta}=\\nabla(\\theta X - Y)^{2}$\n",
    "\n",
    "Para el caso de una línea recta por ejemplo se tiene que encontrar los parámetros de pendiente $m$ y ordenada $b$. El modelo se vuelve entonces\n",
    "\n",
    "$\\hat{y} = mx + b$\n",
    "\n",
    "El error $J(\\theta)$ por mínimos cuadrados entonces es:\n",
    "\n",
    "$J(\\theta) = (mx + b -y)^2$\n",
    "\n",
    "Derivando respecto a un parámetro se obtiene su gradiente, el cual indicará hacia dónde se ha de actualizar el valor en la próxima iteración.\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial m } = -2(y_i-(mx_i+b))x_i$$\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial b } = -2(y_i-(mx_i+b))$$\n",
    "\n",
    "Para actualizar los valores del modelo este gradiente del error total es multiplicado por un factor de aprendizaje $\\alpha$ y finalmente se resta al valor previo.\n",
    "\n",
    "$$m_{t+1} = m_t - \\alpha \\frac{1}{L} \\sum (\\hat{y}-y_i)x_i$$\n",
    "$$b_{t+1} = b_t - \\alpha \\frac{1}{L} \\sum (\\hat{y}-y_i)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros a estimar\n",
    "m = 10000\n",
    "b = 2\n",
    "\n",
    "# Hiperparámetros\n",
    "iteraciones = 500\n",
    "alpha = 1e-4\n",
    "L = 100\n",
    "\n",
    "X = np.arange(L)\n",
    "Y = m * X + b\n",
    "\n",
    "\n",
    "\n",
    "class regresion_simple:\n",
    "    \"\"\"\n",
    "    Crea un modelo de regresión de dos parámetros mediante el método de gradiente descendiente por bloque\n",
    "    \n",
    "    Parámetros\n",
    "    -------\n",
    "    alpha: factor de aprendizaje\n",
    "    tol: tolerancia para finalización temprana\n",
    "    \n",
    "    -------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.001, tol=0.01):\n",
    "        \n",
    "        # Inicializar valores aleatorios\n",
    "        self.m = np.random.random()\n",
    "        self.b = np.random.random()\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = self.m * X + self.b\n",
    "        return y\n",
    "    \n",
    "    def train(self, X, Y, iteraciones=100):\n",
    "        '''\n",
    "        Entrena el modelo de regresión\n",
    "        \n",
    "        Parámetros\n",
    "        ---------\n",
    "        X: Ejemplos de entrada\n",
    "        \n",
    "        Y: Ejemplos de salida esperada\n",
    "        \n",
    "        iteraciones: número de iteraciones para el entrenamiento\n",
    "        ------\n",
    "        '''\n",
    "        \n",
    "        aprendizaje = []\n",
    "        L = len(X)\n",
    "        for i in range(iteraciones):\n",
    "        \n",
    "            # Hacer estimación inicial\n",
    "            y_e = self.predict(X)\n",
    "\n",
    "            # Calculo de gradientes\n",
    "            gradiente_b = (y_e - Y)/L\n",
    "            gradiente_m = gradiente_b * X\n",
    "            \n",
    "            #Actualizar valores\n",
    "            self.m = self.m - self.alpha * np.sum(gradiente_m)\n",
    "            self.b = self.b - self.alpha * np.sum(gradiente_b)\n",
    "            \n",
    "            aprendizaje.append(np.sum(gradiente_b))\n",
    "            if abs(np.sum(gradiente_b)) < self.tol:\n",
    "                break\n",
    "                \n",
    "        return(aprendizaje)\n",
    "            \n",
    "          \n",
    "# Entrenamiento\n",
    "modelo = regresion_simple(alpha=alpha)\n",
    "aprendizaje = modelo.train(X, Y, iteraciones=iteraciones)\n",
    "\n",
    "\n",
    "y_e = modelo.predict(X)\n",
    "print('El valor de pendiete estimado es de: ',modelo.m)\n",
    "print('El valor de ordenada estimado es de: ', modelo.b)\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(X, Y)\n",
    "plt.plot(X, y_e, '--')\n",
    "plt.legend(['Original', 'Estimada'])\n",
    "\n",
    "# Curva de aprendizaje\n",
    "plt.subplot(122)\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(aprendizaje)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo para el set de peso/altura\n",
    "\n",
    "Un ejemplo simple que se puede realizar con este tipo de regresión es el de predecir el el peso de una persona dada su altura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "wh = pd.read_csv('data/weight-height.csv')\n",
    "wh.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh.plot.scatter(x='Height',\n",
    "                y='Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_peso = regresion_simple(alpha=5e-6, tol=0.0001)\n",
    "\n",
    "aprendizaje = predictor_peso.train(wh.Height.as_matrix(),\n",
    "                         wh.Weight.as_matrix(),\n",
    "                        iteraciones=1000)\n",
    "\n",
    "\n",
    "peso_prediccion = predictor_peso.predict(wh.Height.as_matrix())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(wh['Height'],\n",
    "                wh['Weight'], alpha=0.2)\n",
    "\n",
    "plt.plot(wh['Height'], peso_prediccion,\n",
    "        c='r')\n",
    "\n",
    "# Curva de aprendizaje\n",
    "plt.subplot(122)\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(aprendizaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regresion_estocastica:\n",
    "    \"\"\"\n",
    "    Crea un modelo de regresión de dos parámetros mediante el método de gradiente descendiente estocastica\n",
    "    \n",
    "    Parámetros\n",
    "    -------\n",
    "    alpha: factor de aprendizaje\n",
    "    tol: tolerancia para finalización temprana\n",
    "    \n",
    "    -------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.001, tol=0.01):\n",
    "        \n",
    "        # Inicializar valores aleatorios\n",
    "        self.m = np.random.random()\n",
    "        self.b = np.random.random()\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = self.m * X + self.b\n",
    "        return y\n",
    "    \n",
    "    def train(self, X, Y, iteraciones=100):\n",
    "        '''\n",
    "        Entrena el modelo de regresión\n",
    "        \n",
    "        Parámetros\n",
    "        ---------\n",
    "        X: Ejemplos de entrada\n",
    "        \n",
    "        Y: Ejemplos de salida esperada\n",
    "        \n",
    "        iteraciones: número de iteraciones para el entrenamiento\n",
    "        ------\n",
    "        '''\n",
    "        \n",
    "        aprendizaje = []\n",
    "        L = len(X)\n",
    "        for i in range(iteraciones):\n",
    "            for j in range(L):\n",
    "                # Hacer estimación inicial\n",
    "                y_e = self.predict(X[j])\n",
    "\n",
    "                # Calculo de gradientes\n",
    "                gradiente_b = (y_e - Y[j])/L\n",
    "                gradiente_m = gradiente_b * X[j]\n",
    "\n",
    "                #Actualizar valores\n",
    "                self.m = self.m - self.alpha * np.sum(gradiente_m)\n",
    "                self.b = self.b - self.alpha * np.sum(gradiente_b)\n",
    "\n",
    "                aprendizaje.append(np.sum(gradiente_b))\n",
    "                if abs(np.sum(gradiente_b)) < self.tol:\n",
    "                    break\n",
    "\n",
    "        return(aprendizaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_peso_2 = regresion_estocastica(alpha=5e-4, tol=0.0001)\n",
    "\n",
    "aprendizaje = predictor_peso_2.train(wh.Height.as_matrix(),\n",
    "                         wh.Weight.as_matrix(),\n",
    "                        iteraciones=5000)\n",
    "peso_prediccion = predictor_peso_2.predict(wh.Height.as_matrix())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(wh['Height'],\n",
    "                wh['Weight'], alpha=0.2)\n",
    "\n",
    "plt.plot(wh['Height'], peso_prediccion,\n",
    "        color='r')\n",
    "\n",
    "# Curva de aprendizaje\n",
    "plt.subplot(122)\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(aprendizaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regresion_mini_batch:\n",
    "    \"\"\"\n",
    "    Crea un modelo de regresión de dos parámetros mediante el método de gradiente descendiente estocastica\n",
    "    \n",
    "    Parámetros\n",
    "    -------\n",
    "    alpha: factor de aprendizaje\n",
    "    tol: tolerancia para finalización temprana\n",
    "    \n",
    "    -------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.001, tol=0.01):\n",
    "        \n",
    "        # Inicializar valores aleatorios\n",
    "        self.m = np.random.random()\n",
    "        self.b = np.random.random()\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = self.m * X + self.b\n",
    "        return y\n",
    "    \n",
    "    def train(self, X, Y, iteraciones=100, batch_size=32):\n",
    "        '''\n",
    "        Entrena el modelo de regresión\n",
    "        \n",
    "        Parámetros\n",
    "        ---------\n",
    "        X: Ejemplos de entrada\n",
    "        \n",
    "        Y: Ejemplos de salida esperada\n",
    "        \n",
    "        iteraciones: número de iteraciones para el entrenamiento\n",
    "        \n",
    "        batch_size: tamaño del bloque de datos a procesar\n",
    "        ------\n",
    "        '''\n",
    "        \n",
    "        aprendizaje = []\n",
    "        L = len(X)\n",
    "        for i in range(iteraciones):\n",
    "            for j in range(0, L - batch_size, batch_size):\n",
    "                # Hacer estimación inicial\n",
    "                y_e = self.predict(X[j: j + batch_size])\n",
    "                # Calculo de gradientes\n",
    "                gradiente_b = (y_e - Y[j: j + batch_size])/L\n",
    "                gradiente_m = gradiente_b * X[j: j + batch_size]\n",
    "\n",
    "                #Actualizar valores\n",
    "                self.m = self.m - self.alpha * np.sum(gradiente_m)\n",
    "                self.b = self.b - self.alpha * np.sum(gradiente_b)\n",
    "\n",
    "                aprendizaje.append(np.sum(gradiente_b))\n",
    "                if abs(np.sum(gradiente_b)) < self.tol:\n",
    "                    break\n",
    "\n",
    "        return(aprendizaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_peso_3 = regresion_mini_batch(alpha=5e-3, tol=0.0001)\n",
    "\n",
    "aprendizaje = predictor_peso_3.train(wh.Height.as_matrix(),\n",
    "                         wh.Weight.as_matrix(),\n",
    "                        iteraciones=100, batch_size=15)\n",
    "peso_prediccion = predictor_peso_3.predict(wh.Height.as_matrix())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(wh['Height'],\n",
    "                wh['Weight'], alpha=0.2)\n",
    "\n",
    "plt.plot(wh['Height'], peso_prediccion,\n",
    "        color='r')\n",
    "\n",
    "# Curva de aprendizaje\n",
    "plt.subplot(122)\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(aprendizaje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logística\n",
    "\n",
    "La regresión logística es una técnica de regresión usada para clasificación. En esta se tienen las mismas entradas que en el caso de una regresión lineal, pero el resultado de la ponderación de valores es evaluado con la función _logit_ o sigmoide. Esta se puede visualizar como una versión suavizada de la función escalón. Y está definida como:\n",
    "\n",
    "$$ f(x)=\\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "Tiene como características poseer un rango de 0 a 1, ser siempre positiva y cuando $x=0$ tiene un valor de 0.5. Es decir, puede ser interpretada como una función cumulativa de probabilidad que nos devolverá un valor entre 0 y 1 para la probabilidad de pertenencia a una determinada clase. Para realizar la regresión se ha de hacer una binarización de los datos en donde la salida esperada para una de las clases se denota como 0 mientras que la otra como un 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-8,8, 100)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "Y = sigmoid(X)\n",
    "plt.plot(X,Y)\n",
    "plt.axvline(0, linestyle='--', color='r')\n",
    "plt.axhline(0.5, linestyle='--', color='r')\n",
    "plt.title(r\"$f(x)=\\frac{1}{1+e^{-x}}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso la función de error se calcula basado en la entropía cruzada, la cual está definida como:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{L} \\sum [y_i \\log(\\hat{y}_i) + (1-y_i) log(1-\\hat{y}_i)]$$\n",
    "\n",
    "La cual calcula el error de clasificación con la probabilidad de pertener a una clase dada la contraria.\n",
    "\n",
    "Gracias a los logaritmos en la función de costo es evidente que se obtener la función de actualización dado el error en la función de costo como:\n",
    "\n",
    "$$\\frac{1}{L} \\sum (h_{\\theta}(x_i)-y_i)x_j$$\n",
    "\n",
    "Ejercicio para el lector: Demostrar que la derivada de la función sigmoide $s(x)$ se puede expresar como $s'(x) = s(x)(1-s(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear artificialmente distribuciones\n",
    "\n",
    "mu_1 = 0\n",
    "mu_2 = 3.5\n",
    "\n",
    "def distribuciones(mu_1, mu_2, sz=1000, plot=False):\n",
    "    sz_1 = sz\n",
    "    sz_2 = sz\n",
    "\n",
    "    # Distribuciones\n",
    "    X = np.random.randn(sz) + mu_1\n",
    "    X = np.r_[X, np.random.randn(sz) + mu_2]\n",
    "\n",
    "    target = np.zeros(sz_1)\n",
    "    target = np.r_[target, np.ones(sz_2)]\n",
    "    \n",
    "    if plot:\n",
    "        _ = plt.hist(X[:sz], bins=20,\n",
    "                    alpha=0.7)\n",
    "        _ = plt.hist(X[sz:sz + sz], bins=20,\n",
    "                    alpha=0.7)\n",
    "        plt.legend(['Clase 0', 'Clase 1'])\n",
    "        plt.show()\n",
    "    return X, target\n",
    "\n",
    "X, target = distribuciones(mu_1, mu_2,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "class regresion_logistica:\n",
    "    \"\"\"\n",
    "    Crea un modelo de regresión logística de dos parámetros mediante \n",
    "    el método de gradiente descendiente usando entropía cruzada \n",
    "    \n",
    "    Parámetros\n",
    "    -------\n",
    "    alpha: factor de aprendizaje\n",
    "    tol: tolerancia para finalización temprana\n",
    "    \n",
    "    -------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.001, tol=0.01):\n",
    "        \n",
    "        # Inicializar valores aleatorios\n",
    "        self.m = np.random.random()\n",
    "        self.b = np.random.random()\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = sigmoid(self.m * X + self.b)\n",
    "        return y\n",
    "    \n",
    "    def train(self, X, Y, iteraciones=100, batch_size=32, shuf=False):\n",
    "        '''\n",
    "        Entrena el modelo de regresión\n",
    "        \n",
    "        Parámetros\n",
    "        ---------\n",
    "        X: Ejemplos de entrada\n",
    "        \n",
    "        Y: Vector de clases\n",
    "        \n",
    "        iteraciones: número de iteraciones para el entrenamiento\n",
    "        \n",
    "        batch_size: tamaño del bloque de datos a procesar\n",
    "        \n",
    "        shuffle: Mezclar los datos\n",
    "        ------\n",
    "        '''\n",
    "        \n",
    "        if shuf:\n",
    "            X, Y = shuffle(X, Y)\n",
    "            print(X)\n",
    "        aprendizaje = []\n",
    "        L = len(X)\n",
    "        for i in range(iteraciones):\n",
    "            for j in range(0, L - batch_size + 1, batch_size):\n",
    "                # Hacer estimación inicial\n",
    "                y_e = self.predict(X[j: j + batch_size])\n",
    "                #print(Y[j: j + batch_size])\n",
    "                # Calculo de gradientes\n",
    "                gradiente_b = (y_e - Y[j: j + batch_size])/L\n",
    "                gradiente_m = gradiente_b * X[j: j + batch_size]\n",
    "\n",
    "                #Actualizar valores\n",
    "                self.m = self.m - self.alpha * np.sum(gradiente_m)\n",
    "                self.b = self.b - self.alpha * np.sum(gradiente_b)\n",
    "\n",
    "                aprendizaje.append(np.sum(gradiente_b))\n",
    "                if abs(np.sum(gradiente_b)) < self.tol:\n",
    "                    break\n",
    "\n",
    "        return(aprendizaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = regresion_logistica(alpha=0.7)\n",
    "aprendizaje = clasificador.train(X, target, iteraciones=10000,\n",
    "                                 batch_size=32, shuf=True)\n",
    "\n",
    "z = np.linspace(X.min()-1, X.max()+1, 100)\n",
    "\n",
    "zz = clasificador.predict(z)\n",
    "\n",
    "sz = 1000\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(121)\n",
    "_ = plt.hist(X[:sz], bins=20,\n",
    "            density=True)\n",
    "_ = plt.hist(X[sz:sz * 2], bins=20,\n",
    "            density=True)\n",
    "plt.plot(z, zz>0.5, linewidth=5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(aprendizaje)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar la tabla de confusión\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Hacer predicción\n",
    "predicted = (clasificador.predict(X) > 0.5) * 1.0\n",
    "\n",
    "print(confusion_matrix(target, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una tabla de confusión nos da los resultados de la clasificación en términos de la clase verdadera a la cual pertenece un elemento y la clase en la cual fue asignada.\n",
    "\n",
    "\n",
    "Algunas métricas para evaluar el desempeño de un clasificador son las siguientes:\n",
    "\n",
    "$$Precision = \\frac{TP}{TP+TF}$$\n",
    "\n",
    "$$Recall = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$F1 = 2 * \\frac{Precision*Recall}{Precision+Recall}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(target, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neuronales Artificiales\n",
    "\n",
    "Las neuronas artificiales son modelos matemáticos que intentan simular el funcionamiento de las neuronas en el cerebro. Una neurona biológica está compuesta de un núcleo, axón y dendritas.\n",
    "<img src='Class5D/Neurona_jarroba.png' height=\"60\" width=\"180\">\n",
    "\n",
    "<br>\n",
    "Las dendritas cumplen la función de recibir la información proveniente de otras células y transmitirla al núcleo.    \n",
    "El núcleo es el encargado de evaluar si debe o no emitir un estímulo ante la información recibida.  \n",
    "El axón a su vez debe transmitir el estímulo del núcleo a otras células, las cuales pueden o no ser otras neuronas.\n",
    "\n",
    "\n",
    "### Perceptrón\n",
    "\n",
    "Hoy en día existen una gran cantidad de algoritmos basados en redes neuronales, en este curso verémos el más sencillo de ellos, el perceptrón. El perceptrón es un modelo matemático basado en regresión lineal, en este se tiene un vector $X$, de dimensión $K$ de entrada, cada uno de los elementos del vector es ponderado por un vector de pesos, el cual por consecuencia también es de dimensión $K$. Además de esto se tiene un valor de _bias_ $b$, el cual es una constante que se suma a la ecuación.\n",
    "\n",
    "El modelo se puede definir matemáticamente como:\n",
    "\n",
    "$$ Y(X) =\\sum_{n=1}^N w_nx_n +b $$\n",
    "\n",
    "Donde $x_n$ son elementos del vector $X$ y $w_n$ son elementos del vector $W$.\n",
    "\n",
    "El entrenamiento de estas es similar al de los casos de regresión vistos previamente, es por esto que una neurona puede ser usada para ambos tipos de tareas, regresión lineal y clasificación.\n",
    "\n",
    "## Perceptrón Multicapa\n",
    "\n",
    "El aumento en poder computacional en los años recientes permitió el desarrollo de arquitecturas más avanzadas de neuronas, el caso de la perceptrón multicapa se tiene un arreglo en el cual se definen un número de capas y de las neuronas que habrá en cada una. Cada capa recibe la información de la inmediatamente anterior logrando relaciones entre variables más complejas y simplificando la tarea de extracción de características. Cada una de las capas puede poseer funciones de activación diferentes y junto a los parámetros de número de capas y neuronas en cada una permite la creación de arquitectura de redes neuronales.\n",
    "\n",
    "El modelo de una capa de red neuronal se puede definir como:\n",
    "\n",
    "\n",
    "$$ Z _1= f(XW + B) $$\n",
    "\n",
    "Donde $Z$ es la salida de la capa, la cual tendrá la misma dimensión que las neuronas en esa capa, es decir tenemos una respuesta de cada una, $X$ es de nuevo el vector de características de entrada de dimensión $N$, $W$ es una matriz de dimensiones $N$ por $M$, donde $N$ es nuevamente la cantidad de elementos de entrada en el vector $X$ y $M$ la cantidad de neuronas en esa capa,  $B$ es un vector de valores de _bias_ y $f$ es la función de activación seleccionada para esa capa.\n",
    "\n",
    "Al pasar a otra capa se tiene entonces la salida siguiente:\n",
    "\n",
    "$$ Z_2  = g(Z_1 W_2 + B_2)$$\n",
    "\n",
    "Donde en este caso el vector de entrada $X$ ha sido sustituido por el vector $Z_1$, correspondiente a la salida de la capa anterior y la función $g$ la cual puede o no ser la misma que la de la capa anterior. Nuevamente las dimensiones de la matriz de pesos $W_2$ y el vector de bias $B_2$ se proponen de manera que correspondan a las nuevas dimensiones de neuronas y entradas.\n",
    "\n",
    "El entrenamiento de estas se hace por medio de un algoritmo llamado Backpropagation, en el cual se hace uso de la regla de la cadena para estimar el gradiente para los parámetros e cada una de las neuronas que conforman la red neuronal. Es decir se tiene la derivada parcial del error dependiente de cada parámetro. Es por esto que primero se ha de obtener la derivda del error de la última capa, de ahí el nombre de backpropagation.\n",
    "\n",
    "El algoritmo de backpropagation para dos capas entonces puede ser definido como:\n",
    "\n",
    "$$\\frac{\\partial g}{\\partial w} = \\frac{\\partial g}{\\partial f} \\frac{\\partial f}{\\partial w}$$\n",
    "\n",
    "Es frecuente encontrar en la literatura a la expresión $\\frac{\\partial g}{\\partial f}$ simplificada como $\\delta_i$ donde $i$ representa el número de capa. Expandiendo el algoritmo para varias capas se tiene entonces:\n",
    "\n",
    "$$\\frac{\\partial g}{\\partial w} = \\prod_{i=M-1}^k \\delta_i \\frac{\\partial f}{\\partial w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "\n",
    "Keras es un framework para trabajar con redes neuronales desarrollado por François Chollet https://twitter.com/fchollet, esta permite la creación de arquitecturas complejas para investigación y desarrollo, así como implementaciones en la industria. Es una librería basada en tensores, por lo que numpy no es suficiente para realizar los cálculos, es por esto que puede ser usado con alguna de los siguientes backends:\n",
    "\n",
    "* Theano: librería open source desarrollada colectivamente.\n",
    "* TensorFlow: librería desarrollada por Google para Machine Learning.\n",
    "* CNTK: librería desarrollada por Microsoft para Machine Learning.\n",
    "\n",
    "En este curso se usará TensorFlow como el backend de keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo distribuciones\n",
    "sz = 1000\n",
    "X, Y = distribuciones(0, 3, sz=sz)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(1,), activation='sigmoid'))\n",
    "model.compile(Adam(lr=0.1), 'binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "History = model.fit(X, Y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(X.min()-1, X.max()+1, 100)\n",
    "zz = model.predict(z)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "_ = plt.hist(X[:sz], bins=20,\n",
    "            density=True)\n",
    "_ = plt.hist(X[sz:sz + sz], bins=20,\n",
    "            density=True)\n",
    "plt.plot(z, zz>0.5, linewidth=5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(History.history['loss'])\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Entropía cruzada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(model.predict(X)>0.5, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo pesos\n",
    "\n",
    "height = wh.Height.as_matrix()\n",
    "weight = wh.Weight.as_matrix()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "model.compile(Adam(lr=0.5), 'mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "History = model.fit(height, weight, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(height.min()-1, height.max()+1, 1000)\n",
    "zz = model.predict(z)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(height, weight)\n",
    "plt.plot(z, zz, color='r')\n",
    "plt.xlabel('Altura')\n",
    "plt.ylabel('Peso')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(History.history['loss'])\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Error cuadrático medio')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo IRIS\n",
    "import seaborn as sn\n",
    "iris = pd.read_csv('data/iris.csv')\n",
    "sn.pairplot(iris, hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.as_matrix()[:, :4]\n",
    "print(X[:5])\n",
    "Y = iris.as_matrix()[:, 4]\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar a numerico\n",
    "target = np.zeros_like(Y)\n",
    "for i, c in enumerate(iris.species.unique()):\n",
    "    target[Y==c] = i\n",
    "\n",
    "target = np.uint8(target)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar a categorico\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_cat = to_categorical(target)\n",
    "print(y_cat[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(3, input_shape=(4,), activation='softmax'))\n",
    "model.compile(Adam(lr=0.1),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y_cat, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_categorical = model.predict(X)\n",
    "print(predicted_categorical[:5])\n",
    "classes = np.uint8(np.argmax(predicted_categorical, axis=1))\n",
    "print(classes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(target, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando un perceptron multicapa\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(4,), activation='relu'))\n",
    "model.add(Dense(3, input_shape=(4,), activation='softmax'))\n",
    "model.compile(Adam(lr=0.1),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y_cat, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_categorical = model.predict(X)\n",
    "classes = np.uint8(np.argmax(predicted_categorical, axis=1))\n",
    "print(classification_report(target, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo usando PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sn\n",
    "\n",
    "PCA_iris = PCA(n_components=2)\n",
    "\n",
    "Xnueva = PCA_iris.fit_transform(X)\n",
    "iris_reducida = pd.DataFrame(Xnueva,\n",
    "                             columns=['PC1', 'PC2'])\n",
    "\n",
    "iris_reducida['species'] = iris.species\n",
    "sn.pairplot(iris_reducida, hue='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo usando las componentes de PCA\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(3, input_shape=(4,), activation='softmax'))\n",
    "model.compile(Adam(lr=0.1),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(Xnueva, y_cat, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puntos = 200\n",
    "x = np.linspace(Xnueva[:,0].min()-1,\n",
    "                Xnueva[:,0].max()+1, puntos)\n",
    "y = np.linspace(Xnueva[:,1].min()-1,\n",
    "                Xnueva[:,1].max()+1, puntos)\n",
    "\n",
    "xv, yv = np.meshgrid(x,y)\n",
    "z = np.c_[xv.ravel(), yv.ravel()]\n",
    "#np.shape(z)\n",
    "zz = np.argmax(model.predict(z), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(Xnueva[:,0], Xnueva[:,1], c=target)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.contourf(x, y, zz.reshape(puntos,puntos), alpha=0.3)\n",
    "plt.contour(x, y, zz.reshape(puntos,puntos), alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "Usar todas las herramientas vistas en el curso para visualizar en primer lugar y realizar algun otro proceso matemático de selección y manipulación de datos si es necesario.\n",
    "\n",
    "1.-Realizar un modelo que prediga los precios de las casa para el dataset de Boston 'data/housing-data.csv'. \n",
    "\n",
    "2.-Realizar un modelo que prediga el número de pasajeros que se esperan en el próximo mes usando el dataset de aeropuertos 'data/international-airline-passengers.csv'. Tip: utilizar pd.DatetimeIndex.month  y pd.DatetimeIndex.year\n",
    "\n",
    "3.- Realizar un modelo que prediga si una persona sobrevive o no usando el dataset de titanic. 'data/titani-train.csv'. Dado que la columna de género no es un dato numérico requiere que se creen dos columnas con cada uno de ellos y si pertenece o no al genero de la columna."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
